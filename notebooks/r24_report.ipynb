{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import sys\n",
    "import plotly.io as pio\n",
    "import pandas as pd # type: ignore\n",
    "import random\n",
    "import xarray as xr # type: ignore\n",
    "pio.renderers.default = \"png\"\n",
    "sys.path.append('../')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the voting data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nqg_model.experiment import full_historical_data_run_plus_counterfactual\n",
    "from nqg_model.load_data import retrieve_id_maps, retrieve_vote_data\n",
    "\n",
    "## Simulation Originated Data\n",
    "round_no = 24\n",
    "folder_path = f'../data/r{round_no}/'\n",
    "\n",
    "maps = retrieve_id_maps(folder_path)\n",
    "\n",
    "sim_backtest_df = full_historical_data_run_plus_counterfactual(folder_path=folder_path, round_no=24)\n",
    "sim_df = pd.concat([sim_backtest_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# sim_df.to_pickle(f\"../data/r{round_no}/sim_results/results_{datetime.now()}.pkl.zip\")\n",
    "\n",
    "## Empirical Data\n",
    "df = pd.read_csv(f'../data/r{round_no}/votes.csv')\n",
    "df['user'] = df['user'].astype(str).map(lambda x: maps.UserID2Wallet.get(x, f'unknown-{x}'))\n",
    "df['project'] = df['project'].astype(str).map(lambda x: maps.SubmissionID2Label.get(x, f'unknown-{x}'))\n",
    "df.rename(columns={'user': 'user_ref', 'project': 'submission', 'vote_type': 'vote_type'}, inplace=True)\n",
    "\n",
    "# fixing seed for reproducibility\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the neuron power tensor to render sim_df quickly\n",
    "neuron_power_tensor_df = sim_df[['neuron_power_tensor', 'label']]\n",
    "sim_df.drop(columns=['neuron_power_tensor'], inplace=True)\n",
    "\n",
    "sim_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to process and visualize the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Voting Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "project_voting_summary(df, round_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Engagement Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "user_engagement_overview(df, round_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.vote_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trust Network Analysis\n",
    "- Visualization of trust relationships and their impact on voting results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "trust_network_analysis(sim_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario Cross Comparision\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminology:\n",
    "- Standard Voting: This refers to a voting mechanism wherein each participant possesses equal voting power, and the process does not involve Quorum Delegation.\n",
    "- NQG: An acronym for Neural Quorum Governance, a system in which voting weight is determined through a combination of factors such as Trust, Expertise, and Voting History Neurons, in addition to the inclusion of Quorum Delegation.\n",
    "- NG w/o QD: Stands for Neural Governance without Quorum Delegation. In this framework, voting weight is assigned based on Trust, Expertise, and Voting History Neurons; however, it excludes the aspect of Quorum Delegation.\n",
    "- QD w/o NG: Denotes Quorum Delegation without Neural Governance. Under this system, each voter retains an equal vote, but the process allows for the delegation of votes through Quorum Delegation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_map = {\n",
    "    'backtesting': 'NQG',\n",
    "    'no_QD': 'NG w/o QD',\n",
    "    'no_NG': 'QD w/o NG',\n",
    "    'no_NQG': '1 person, 1 vote'}\n",
    "\n",
    "(_, counterfact_df, full_df, ranked_full_df) = get_results_counterfact_full_ranked_dfs(\n",
    "    df, sim_df, scenario_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "counterfactual_heatmap_1(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "counterfactual_heatmap2(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(full_df.reset_index().sort_values('result', ascending=False), x='index', y='result', labels={'result': 'Power', 'index': 'Submission'}).update_layout(height=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_heatmap_3(ranked_full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_heatmap_3(ranked_full_df[['result', '1 person, 1 vote']].sort_values('result'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron Tensor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_power_tensor = neuron_power_tensor_df[neuron_power_tensor_df['label']=='backtesting']\n",
    "neuron_power_tensor=neuron_power_tensor['neuron_power_tensor'].values[1]\n",
    "\n",
    "vote_decision_matrix=pd.DataFrame(sim_df['vote_decision_matrix'][1])\n",
    "\n",
    "vote_tensor = xr.DataArray(vote_decision_matrix, dims=['project', 'user'])\n",
    "\n",
    "# Ensure matching coordinates (This step is crucial for correct broadcasting and multiplication)\n",
    "vote_tensor = vote_tensor.assign_coords(project=neuron_power_tensor.project.values, user=neuron_power_tensor.user.values)\n",
    "\n",
    "# Broadcast 'vote_tensor' across the 'layer' and 'neuron' dimensions\n",
    "vote_tensor = vote_tensor.expand_dims({'layer': neuron_power_tensor.layer, 'neuron': neuron_power_tensor.neuron}, axis=[-2, -1])\n",
    "\n",
    "# Now multiply\n",
    "resulting_tensor = neuron_power_tensor * vote_tensor\n",
    "\n",
    "# Select layers\n",
    "layer1 = resulting_tensor.sel(layer=0)\n",
    "layer2 = resulting_tensor.sel(layer=1)\n",
    "\n",
    "# Sum across neurons in layer1\n",
    "layer1_sum = layer1.sum(dim='neuron')\n",
    "\n",
    "# Product across neurons in layer2\n",
    "layer2_prod = layer2.prod(dim='neuron')\n",
    "\n",
    "# Calculate final power\n",
    "final_power = abs(layer2_prod) * layer1_sum / abs(layer1_sum)\n",
    "final_power=final_power.drop_vars('layer')\n",
    "\n",
    "# Step 2: Adjusted Calculations\n",
    "# NQG_wo_L1_trust_score: Assume layer1 neuron1 is 0\n",
    "layer1_wo_neuron1 = layer1.copy()\n",
    "layer1_wo_neuron1.loc[dict(neuron=0)] = 0  # Set neuron1 to 0\n",
    "\n",
    "NQG_wo_L1_trust_score = (layer1_wo_neuron1.sum(dim='neuron')) * (layer2.prod(dim='neuron'))\n",
    "\n",
    "# NQG_wo_L1_reputation_score: Assume layer1 neuron2 is 0\n",
    "layer1_wo_neuron2 = layer1.copy()\n",
    "layer1_wo_neuron2.loc[dict(neuron=1)] = 0  # Set neuron2 to 0\n",
    "NQG_wo_L1_reputation_score = (layer1_wo_neuron2.sum(dim='neuron')) * (layer2.prod(dim='neuron'))\n",
    "\n",
    "# NQG_wo_L2_past_round: Assume layer2 neuron1 is 1\n",
    "layer2_wo_neuron1 = layer2.copy()\n",
    "layer2_wo_neuron1.loc[dict(neuron=0)] = 1  # Set neuron1 to 1\n",
    "NQG_wo_L2_past_round = (layer1.sum(dim='neuron')) * (layer2_wo_neuron1.prod(dim='neuron'))\n",
    "\n",
    "# Convert these xarray operations into a new DataArray or Dataset\n",
    "metrics = xr.Dataset({\n",
    "    'NQG': final_power,\n",
    "    'NQG_wo_L1_trust_score': NQG_wo_L1_trust_score,\n",
    "    'NQG_wo_L1_reputation_score': NQG_wo_L1_reputation_score,\n",
    "    'NQG_wo_L2_past_round': NQG_wo_L2_past_round\n",
    "})\n",
    "\n",
    "# Aggregate by user (summing over projects for simplicity, adjust as needed)\n",
    "final_metrics = metrics.sum(dim='user')\n",
    "\n",
    "\n",
    "# Creating the DataFrame\n",
    "ranked_metrics = final_metrics.to_dataframe().reset_index()\n",
    "\n",
    "# Applying the rank method to each column (excluding 'project') in descending order\n",
    "ranked_df = ranked_metrics.set_index('project').rank(method='min', ascending=False)\n",
    "\n",
    "# Sum the ranks across all columns to get a total ranking score for sorting\n",
    "ranked_df['total_rank'] = ranked_df.sum(axis=1)\n",
    "\n",
    "# Sort by the total rank and drop the auxiliary column\n",
    "ranked_df = ranked_df.sort_values(by='total_rank').drop(columns='total_rank')\n",
    "\n",
    "# Reset index to bring the 'project' column back\n",
    "ranked_df = ranked_df.reset_index()\n",
    "\n",
    "\n",
    "# # Applying the rank method, which defaults to ranking in descending order\n",
    "# ranked_df = ranked_metrics.rank(method='min', ascending=False)\n",
    "# ranked_df['project'] = ranked_metrics['project']  # Insert the project names back into the DataFrame\n",
    "# ranked_df = ranked_df[['project', 'NQG', 'NQG_wo_L1_trust_score', 'NQG_wo_L1_reputation_score', 'NQG_wo_L2_past_round']]\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(ranked_df.set_index('project').astype(float), annot=True, cmap='viridis', fmt=\".2f\")\n",
    "plt.title('Heatmap of Neuron Power Ranks by Projects')\n",
    "plt.xlabel('Neuron counterfactuals')\n",
    "plt.ylabel('Projects')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
