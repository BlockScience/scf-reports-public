{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import plotly.io as pio\n",
    "import pandas as pd # type: ignore\n",
    "import random\n",
    "pio.renderers.default = \"png\"\n",
    "sys.path.append('../')\n",
    "import xarray as xr # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import seaborn as sns # type: ignore\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the voting data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nqg_model.experiment import full_historical_data_run_plus_counterfactual\n",
    "from nqg_model.load_data import retrieve_id_maps, retrieve_vote_data\n",
    "from helper_functions import generate_users_file, generate_submissions_file\n",
    "## Simulation Originated Data\n",
    "round_no = 27\n",
    "folder_path = f'../data/r{round_no}/'\n",
    "\n",
    "votes = pd.read_csv(folder_path + 'votes.csv')\n",
    "\n",
    "# if users.csv not in folder_path, generate it\n",
    "if not os.path.exists(folder_path + 'users.csv'):\n",
    "    generate_users_file(votes, folder_path)\n",
    "\n",
    "# if submissions.csv not in folder_path, generate it\n",
    "if not os.path.exists(folder_path + 'submissions.csv'):\n",
    "    generate_submissions_file(votes, folder_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps = retrieve_id_maps(folder_path)\n",
    "\n",
    "(sim_backtest_df, df) = full_historical_data_run_plus_counterfactual(folder_path=folder_path, round_no=round_no)\n",
    "sim_df = pd.concat([sim_backtest_df], ignore_index=True)\n",
    "\n",
    "df.rename(columns={'user': 'user_ref', 'project': 'submission', 'vote_type': 'vote_type'}, inplace=True)\n",
    "\n",
    "# fixing seed for reproducibility\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the neuron power tensor to render sim_df quickly\n",
    "neuron_power_tensor_df = sim_df[['neuron_power_tensor', 'label']]\n",
    "sim_df.drop(columns=['neuron_power_tensor'], inplace=True)\n",
    "\n",
    "sim_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to process and visualize the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Voting Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique Submissions:\\t{df.submission.nunique()}\")\n",
    "print(f\"Vote Count:\\t{len(df)}\")\n",
    "print(f\"Delegated Votes Count:\\t{(df.vote_type == 'Delegate').sum()}\")\n",
    "print(f\"'Yes' Direct Votes Count:\\t{(df.vote_type == 'Yes').sum()}  ({(df.vote_type == 'Yes').sum() / ((df.vote_type == 'Yes') | (df.vote_type == 'No')).sum() :.1%} of direct)\")\n",
    "print(f\"'No' Direct Votes Count :\\t{(df.vote_type == 'No').sum()} ({(df.vote_type == 'No').sum() / ((df.vote_type == 'Yes') | (df.vote_type == 'No')).sum() :.1%} of direct)\")\n",
    "print(\"---\")\n",
    "print(f\"Total Voter Count: {df.user_ref.nunique()}\")\n",
    "print(f\"Voters that did choose to delegate to at least 1 project: {df[df.vote_type == 'Delegate'].user_ref.nunique()}\")\n",
    "\n",
    "print(\"### Delegation Outcomes ###\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtesting_df = sim_df.query(\"label == 'backtesting' and timestep == 1\").iloc[0]\n",
    "from nqg_model.types import ProjectAction, Vote\n",
    "\n",
    "\n",
    "\n",
    "n_delegate_yes = 0\n",
    "n_delegate_no = 0\n",
    "n_delegate_abstain = 0\n",
    "for user, actions in backtesting_df.action_matrix.items():\n",
    "    for submission, action in actions.items():\n",
    "        if action == ProjectAction.Delegate:\n",
    "            outcome = backtesting_df.vote_decision_matrix[user][submission]\n",
    "\n",
    "            if outcome == Vote.Yes:\n",
    "                n_delegate_yes += 1\n",
    "            elif outcome == Vote.No:\n",
    "                n_delegate_no += 1\n",
    "            elif outcome == Vote.Abstain:\n",
    "                n_delegate_abstain += 1\n",
    "\n",
    "delegate_set = set()\n",
    "for v in backtesting_df.delegatees.values():\n",
    "    delegate_set |= set(v)\n",
    "print(f\"Delegations mapped to `Yes`: {n_delegate_yes}\")\n",
    "print(f\"Delegations mapped to `No`: {n_delegate_no}\")\n",
    "print(f\"Delegations mapped to `Abstain`: {n_delegate_abstain}\")\n",
    "print(f\"Unique Delegates: {len(delegate_set)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "project_voting_summary(df, round_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Engagement Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "user_engagement_overview(df, round_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.vote_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_votes = df.vote_type.value_counts()['Yes']\n",
    "no_votes = df.vote_type.value_counts()['No']\n",
    "delegated_votes = df.vote_type.value_counts()['Delegate']\n",
    "total_votes = yes_votes + no_votes + delegated_votes\n",
    "direct_votes = yes_votes + no_votes\n",
    "perc_yes = yes_votes / direct_votes*100\n",
    "perc_no = no_votes / direct_votes*100\n",
    "\n",
    "unique_delegators = df[df.vote_type == 'Delegate'].user_ref.nunique()\n",
    "print(f\"Total votes: {total_votes}\")\n",
    "print(f\"Yes votes: {yes_votes}\")\n",
    "print(f\"No votes: {no_votes}\")\n",
    "print(f\"Delegated votes: {delegated_votes}\")\n",
    "print(f\"Percentage of Yes votes: {perc_yes}\")\n",
    "print(f\"Percentage of No votes: {perc_no}\")\n",
    "print(f\"Unique users who delegated atleast once: {unique_delegators}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During SCF #27, 13 submissions received 453 direct votes, while 318 were delegated. The direct votes can be split into 326 (72%) `Yes` and 127 (28%) `No` votes. \n",
    "\n",
    "# A total of 41 unique voters participated. Out of these, 29 voters chose to delegate their vote for at least one project\n",
    "\n",
    "\n",
    "print(f'''\n",
    "      During SCF #{round_no}, {df['submission'].nunique()} submissions received {direct_votes} direct votes, while {delegated_votes} were delegated. \n",
    "      The direct votes can be split into {yes_votes} ({perc_yes:.2f}%) `Yes` and {no_votes} ({perc_no:.2f}%) `No` votes. \n",
    "      A total of {df['user_ref'].nunique()} unique voters participated. Out of these, {unique_delegators} voters chose to delegate their vote for at least one project''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vote_summary=df.groupby('user_ref').vote_type.value_counts().unstack().fillna(0).astype(int)\n",
    "user_vote_summary\n",
    "# how many users have 0 in delegate\n",
    "user_vote_summary[user_vote_summary['Delegate']!=0].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trust Network Analysis\n",
    "- Visualization of trust relationships and their impact on voting results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "trust_network_analysis(sim_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario Cross Comparision\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminology:\n",
    "- Standard Voting: This refers to a voting mechanism wherein each participant possesses equal voting power, and the process does not involve Quorum Delegation.\n",
    "- NQG: An acronym for Neural Quorum Governance, a system in which voting weight is determined through a combination of factors such as Trust, Expertise, and Voting History Neurons, in addition to the inclusion of Quorum Delegation.\n",
    "- NG w/o QD: Stands for Neural Governance without Quorum Delegation. In this framework, voting weight is assigned based on Trust, Expertise, and Voting History Neurons; however, it excludes the aspect of Quorum Delegation.\n",
    "- QD w/o NG: Denotes Quorum Delegation without Neural Governance. Under this system, each voter retains an equal vote, but the process allows for the delegation of votes through Quorum Delegation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_map = {\n",
    "    'backtesting': 'NQG',\n",
    "    'no_QD': 'NG w/o QD',\n",
    "    'no_NG': 'QD w/o NG',\n",
    "    'no_NQG': '1 person, 1 vote'}\n",
    "\n",
    "(_, counterfact_df, full_df, ranked_full_df) = get_results_counterfact_full_ranked_dfs(\n",
    "    df, sim_df, scenario_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(full_df.reset_index().sort_values('result', ascending=False), x='index', y='result', labels={'result': 'Power', 'index': 'Submission'}).update_layout(height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "counterfactual_heatmap_1(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "counterfactual_heatmap2(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_heatmap_3(ranked_full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_heatmap_3(ranked_full_df[['result', '1 person, 1 vote']].sort_values('result'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_heatmap_3(ranked_full_df[['NQG', 'NG w/o QD']].sort_values('NQG'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron Power Tensor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_power_tensor = neuron_power_tensor_df[neuron_power_tensor_df['label']=='backtesting']\n",
    "neuron_power_tensor=neuron_power_tensor['neuron_power_tensor'].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_power_tensor.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron_power_tensor = neuron_power_tensor_df[neuron_power_tensor_df['label']=='backtesting']\n",
    "# neuron_power_tensor=neuron_power_tensor['neuron_power_tensor'].values[1]\n",
    "\n",
    "# neuron_power_tensor_neuron0_layer1=neuron_power_tensor.sel(layer='layer_1',neuron='past_round')/neuron_power_tensor.sel(layer='layer_1').sum(dim='neuron') \n",
    "\n",
    "# # Replace NaN values with 0\n",
    "# neuron_power_tensor_neuron0_layer1 = neuron_power_tensor_neuron0_layer1.fillna(0)\n",
    "# # Replace inf values with 0\n",
    "# neuron_power_tensor_neuron0_layer1 = neuron_power_tensor_neuron0_layer1.where(np.isfinite(neuron_power_tensor_neuron0_layer1), 0)\n",
    "\n",
    "# neuron_power_tensor.loc[dict(neuron='past_round', layer='layer_1')] = neuron_power_tensor_neuron0_layer1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron_power_tensor = neuron_power_tensor_df[neuron_power_tensor_df['label']=='backtesting']\n",
    "# neuron_power_tensor=neuron_power_tensor['neuron_power_tensor'].values[1]\n",
    "\n",
    "\n",
    "vote_decision_matrix=pd.DataFrame(sim_df['vote_decision_matrix'][1])\n",
    "\n",
    "vote_tensor = xr.DataArray(vote_decision_matrix, dims=['project', 'user'])\n",
    "\n",
    "# Ensure matching coordinates (This step is crucial for correct broadcasting and multiplication)\n",
    "vote_tensor = vote_tensor.assign_coords(project=neuron_power_tensor.project.values, user=neuron_power_tensor.user.values)\n",
    "\n",
    "# Broadcast 'vote_tensor' across the 'layer' and 'neuron' dimensions\n",
    "vote_tensor = vote_tensor.expand_dims({'layer': neuron_power_tensor.layer, 'neuron': neuron_power_tensor.neuron}, axis=[-2, -1])\n",
    "def calculate_final_metrics(neuron_power_tensor, vote_tensor):\n",
    "    # Now multiply\n",
    "    resulting_tensor = neuron_power_tensor * vote_tensor\n",
    "\n",
    "    # Select layers\n",
    "    layer1 = resulting_tensor.sel(layer='layer_0')\n",
    "    layer2 = resulting_tensor.sel(layer='layer_1')\n",
    "\n",
    "    # Sum across neurons in layer1\n",
    "    layer1_sum = layer1.sum(dim='neuron')\n",
    "\n",
    "    # Product across neurons in layer2\n",
    "    layer2_prod = layer2.prod(dim='neuron')\n",
    "\n",
    "    # Calculate final power\n",
    "    final_power = abs(layer2_prod) * abs(layer1_sum) * (layer2_prod / abs(layer2_prod))\n",
    "    final_power=final_power.drop_vars('layer')\n",
    "\n",
    "    # Step 2: Adjusted Calculations\n",
    "    # NQG_wo_L1_trust_score: Assume layer1 neuron1 is 0\n",
    "    layer1_wo_neuron1 = layer1.copy()\n",
    "    layer1_wo_neuron1.loc[dict(neuron='trust_score')] = 0  # Set neuron1 to 0\n",
    "    NQG_wo_L1_trust_score = (layer1_wo_neuron1.sum(dim='neuron')) * (layer2.prod(dim='neuron'))\n",
    "\n",
    "    # NQG_wo_L1_reputation_score: Assume layer1 neuron2 is 0\n",
    "    layer1_wo_neuron2 = layer1.copy()\n",
    "    layer1_wo_neuron2.loc[dict(neuron='reputation_score')] = 0  # Set neuron2 to 0\n",
    "    NQG_wo_L1_reputation_score = (layer1_wo_neuron2.sum(dim='neuron')) * (layer2.prod(dim='neuron'))\n",
    "\n",
    "    # NQG_wo_L2_past_round: Assume layer2 neuron1 is 1\n",
    "    layer2_wo_neuron1 = layer2.copy()\n",
    "    layer2_wo_neuron1.loc[dict(neuron='past_round')] = 1  # Set neuron1 to 1\n",
    "    NQG_wo_L2_past_round = (layer1.sum(dim='neuron')) * (layer2_wo_neuron1.prod(dim='neuron'))\n",
    "\n",
    "    # Convert these xarray operations into a new DataArray or Dataset\n",
    "    metrics = xr.Dataset({\n",
    "        'NQG': final_power,\n",
    "        'NQG_wo_L1_trust_score': NQG_wo_L1_trust_score,\n",
    "        'NQG_wo_L1_reputation_score': NQG_wo_L1_reputation_score,\n",
    "        'NQG_wo_L2_past_round': NQG_wo_L2_past_round\n",
    "    })\n",
    "\n",
    "    # Aggregate by user (summing over projects for simplicity, adjust as needed)\n",
    "    final_metrics = metrics.sum(dim='user')\n",
    "    return final_metrics\n",
    "\n",
    "\n",
    "final_metrics = calculate_final_metrics(neuron_power_tensor, vote_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(sim_df['per_project_voting'][1]) - final_metrics.to_dataframe()['NQG']\n",
    "# there is some difference of 0.1 and 0.2 for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metrics.to_dataframe()['NQG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_metrics = final_metrics.to_dataframe().reset_index().sort_values('NQG', ascending=False)\n",
    "\n",
    "ranked_df = ranked_metrics.set_index('project').rank(method='min', ascending=False)\n",
    "\n",
    "\n",
    "ranked_df = ranked_df.reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(ranked_df.set_index('project').astype(float), annot=True, cmap='viridis', fmt=\".2f\")\n",
    "plt.title('Heatmap of Neuron Power Ranks by Projects')\n",
    "plt.xlabel('Neuron counterfactuals')\n",
    "plt.ylabel('Projects')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_df = ranked_df.set_index('project')\n",
    "\n",
    "# Calculate the rank differences\n",
    "rank_changes = ranked_df.subtract(ranked_df['NQG'], axis=0)\n",
    "\n",
    "# Calculate absolute rank changes for identifying large movements\n",
    "rank_changes_abs = rank_changes.abs()\n",
    "\n",
    "# Maximum rank change for each project\n",
    "ranked_df['Max_Rank_Change'] = rank_changes_abs.max(axis=1)\n",
    "\n",
    "# Projects with the highest difference in ranks\n",
    "max_change_value = ranked_df['Max_Rank_Change'].max()\n",
    "projects_high_change = ranked_df[ranked_df['Max_Rank_Change'] == max_change_value]\n",
    "\n",
    "# Projects with unchanged ranks across all conditions\n",
    "unchanged_ranks = ranked_df[ranked_df['Max_Rank_Change'] == 0]\n",
    "\n",
    "# Descriptive statistics\n",
    "rank_descriptive_stats = rank_changes_abs.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(\"Projects with Highest Rank Change:\")\n",
    "projects_high_change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nProjects with Unchanged Ranks:\")\n",
    "unchanged_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDescriptive Statistics of Rank Changes:\")\n",
    "rank_descriptive_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing against the JSON files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name, neuron_name=None):\n",
    "    with open(folder_path + file_name) as f:\n",
    "        data = json.load(f)\n",
    "    return pd.DataFrame(list(data.items()), columns=['user', neuron_name])\n",
    "\n",
    "assigned_reputation_neuron = load_data('assigned_reputation_neuron.json', 'reputation_score')\n",
    "prior_voting_history_neuron = load_data('prior_voting_history_neuron.json', 'past_round')\n",
    "trust_graph_neuron = load_data('trust_graph_neuron.json', 'trust_score')\n",
    "# merge on user\n",
    "\n",
    "neuron_data = assigned_reputation_neuron.merge(prior_voting_history_neuron, on='user').merge(trust_graph_neuron, on='user')\n",
    "\n",
    "neuron_data.set_index('user', inplace=True)\n",
    "neuron_data=neuron_data.astype(float)/10**18\n",
    "neuron_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trust_neuron_npt=neuron_power_tensor.mean(dim='project').sel(neuron = 'trust_score', layer= 'layer_0')\n",
    "prior_voting_neuron_npt=neuron_power_tensor.mean(dim='project').sel(neuron = 'past_round', layer= 'layer_1')\n",
    "reputation_neuron_npt=neuron_power_tensor.mean(dim='project').sel(neuron = 'reputation_score', layer= 'layer_0')\n",
    "\n",
    "trust_neuron_npt.name='trust_score'\n",
    "prior_voting_neuron_npt.name='past_round'\n",
    "reputation_neuron_npt.name='reputation_score'\n",
    "\n",
    "trust_neuron_npt=trust_neuron_npt.to_dataframe().reset_index()[['user', 'trust_score']]\n",
    "prior_voting_neuron_npt=prior_voting_neuron_npt.to_dataframe().reset_index()[['user', 'past_round']]\n",
    "reputation_neuron_npt=reputation_neuron_npt.to_dataframe().reset_index()[['user', 'reputation_score']]\n",
    "\n",
    "sim_df_neuron_data = trust_neuron_npt.merge(prior_voting_neuron_npt, on='user').merge(reputation_neuron_npt, on='user')\n",
    "sim_df_neuron_data.sort_values('user', ascending=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df_neuron_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the two dataframes\n",
    "\n",
    "compare_neuron_results=sim_df_neuron_data.merge(neuron_data, on='user', suffixes=('_sim', '_nqg'))\n",
    "compare_neuron_results.set_index('user', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 3 bar charts for each neuron\n",
    "compare_neuron_results[['trust_score_sim', 'trust_score_nqg']].plot(kind='bar', figsize=(10, 6), title='Trust Score Comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# past_round_sim - (trust_sim + reputation_sim) = past_round_nqg\n",
    "\n",
    "compare_neuron_results['raw_past_round_sim']=(compare_neuron_results['past_round_sim'] - (compare_neuron_results['trust_score_sim'] + compare_neuron_results['reputation_score_sim']))\n",
    "compare_neuron_results[['raw_past_round_sim', 'past_round_nqg']].plot(kind='bar', figsize=(10, 6), title='Trust Score Comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_neuron_results[['reputation_score_sim', 'reputation_score_nqg']].plot(kind='bar', figsize=(10, 6), title='Reputation Score Comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "vote_type_map = {ProjectAction.Vote: 'Vote', ProjectAction.Delegate: 'Delegate'}\n",
    "vote_decision_map = {Vote.Yes: 'Yes', Vote.Abstain: 'Abstain', Vote.No: 'No'}\n",
    "\n",
    "for user, user_decisions in backtesting_df.action_matrix.items():\n",
    "    for project, decision in user_decisions.items():\n",
    "        delegation_result = backtesting_df.vote_decision_matrix[user][project]\n",
    "        tally_vote_power = backtesting_df.vote_power_matrix[user][project]\n",
    "\n",
    "\n",
    "        if decision == ProjectAction.Vote:\n",
    "            vote_type = vote_decision_map[backtesting_df.vote_decision_matrix[user][project]]\n",
    "            delegation_result = 'Non_delegated'\n",
    "        elif decision == ProjectAction.Delegate:\n",
    "            vote_type = 'Delegate'\n",
    "            delegation_result = vote_decision_map[backtesting_df.vote_decision_matrix[user][project]]\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "\n",
    "        \n",
    "        records.append({'user_ref': user,\n",
    "                        'submission': project, \n",
    "                        'vote_type': vote_type,\n",
    "                        'delegation_result': delegation_result,\n",
    "                        'tally_vote_power': tally_vote_power})\n",
    "\n",
    "\n",
    "sim_vote_result_df = pd.DataFrame(records)\n",
    "\n",
    "sim_vote_result_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_direct_sim = sim_vote_result_df.query('vote_type == \"Yes\" or vote_type == \"No\"').set_index(['user_ref', 'submission']).sort_index()\n",
    "df_direct_sim.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delegated_sim = sim_vote_result_df.query('vote_type == \"Delegate\"').set_index(['user_ref', 'submission']).sort_index()\n",
    "df_delegated_sim.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delegated = df.copy().query(\"vote_type == 'Delegate'\").set_index(['user_ref', 'submission']).loc[:, df_delegated_sim.columns].sort_index()\n",
    "df_delegated.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_direct = df.copy().query(\"vote_type == 'Yes' or vote_type == 'No'\").set_index(['user_ref', 'submission']).loc[:, df_direct_sim.columns].sort_index()\n",
    "df_direct.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_delegated_sim)\n",
    "print()\n",
    "print(\"Sim df\")\n",
    "print(df_delegated_sim.delegation_result.value_counts())\n",
    "print()\n",
    "print(\"Result df\")\n",
    "print(df_delegated.delegation_result.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sim df\")\n",
    "print(df_direct_sim.vote_type.value_counts())\n",
    "print()\n",
    "print(\"Result df\")\n",
    "print(df_direct.vote_type.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(df_direct.tally_vote_power, df_direct_sim.tally_vote_power, rtol=1e-3,atol=1e-2).mean() == 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert np.isclose(df_delegated.tally_vote_power, df_delegated_sim.tally_vote_power, rtol=1e-3,atol=1e-2).mean() == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.testing.assert_frame_equal(df_delegated, df_delegated_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delegated.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_delegated[df_delegated.delegation_result != df_delegated_sim.delegation_result]\n",
    "joined_df = df_delegated.join(df_delegated_sim, rsuffix=\"_sim\").loc[filtered_df.index]\n",
    "joined_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filtered_df))\n",
    "joined_df.groupby(by=['delegation_result', 'delegation_result_sim']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_usr = 'GCZH4PHHWFNZGZH76UUOZE6X75HTPBOSXSRZXCGUSDGFM2XQE3JC2E5U'\n",
    "proj = 'NAUTA (P2P) SECONDARY MARKET'\n",
    "delegates = backtesting_df.delegatees[src_usr]\n",
    "inds = (df.user_ref.isin(delegates)) & (df.submission == proj)\n",
    "df_int = df[inds]\n",
    "\n",
    "print(\"Sim-originated outcome\")\n",
    "print(df_delegated_sim.loc[(src_usr, proj)])\n",
    "print(\"Results outcome\")\n",
    "print(df_delegated.loc[(src_usr, proj)])\n",
    "\n",
    "df_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
